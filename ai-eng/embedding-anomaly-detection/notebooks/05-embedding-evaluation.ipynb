{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Embedding Evaluation\n\n> **Theory**: See [Part 5: Evaluating Embedding Quality](../part5-embedding-quality.md) for concepts behind embedding evaluation.\n\nEvaluate the quality of your trained embeddings using both quantitative metrics and qualitative inspection.\n\n**What you'll learn:**\n1. Visualize embeddings with t-SNE and UMAP\n2. Compute cluster quality metrics (Silhouette, Davies-Bouldin)\n3. Inspect nearest neighbors to verify semantic similarity\n4. Test embedding robustness to input perturbations\n5. Generate a comprehensive quality report\n\n**Prerequisites:**\n- Embeddings from [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb)\n- Trained model (`tabular_resnet.pt`)\n\n---\n\n## Why Evaluate Embeddings?\n\n**The challenge**: Just because your training loss decreased doesn't mean your embeddings are useful. A model can memorize training data while learning poor representations.\n\n**The solution**: Evaluate from multiple angles:\n- **Quantitative** (objective metrics like Silhouette Score)\n- **Qualitative** (visual inspection, nearest neighbors)\n\nNumbers don't tell the whole story - you need to *look* at your data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, silhouette_samples\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\n\nprint(\"\u2713 All imports successful\")\nprint(\"\\nLibraries loaded:\")\nprint(\"  - NumPy for numerical operations\")\nprint(\"  - Matplotlib for visualization\")\nprint(\"  - Scikit-learn for clustering and metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Embeddings\n\nLoad the embeddings generated in the self-supervised training notebook.\n\n**What you should expect:**\n- Shape: `(N, 128)` - one 128-dimensional vector per OCSF event\n- Values roughly centered around 0\n- No NaN or Inf values\n\n**If you see errors:**\n- `FileNotFoundError`: Run notebook 04 first to generate embeddings\n- Wrong shape: Ensure you're using the correct embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\nembeddings = np.load('../data/embeddings.npy')\n\n# Load original features (for perturbation testing later)\nnumerical = np.load('../data/numerical_features.npy')\ncategorical = np.load('../data/categorical_features.npy')\n\nwith open('../data/feature_artifacts.pkl', 'rb') as f:\n    artifacts = pickle.load(f)\n\nprint(\"Loaded Embeddings:\")\nprint(f\"  Shape: {embeddings.shape}\")\nprint(f\"  Mean: {embeddings.mean():.4f}\")\nprint(f\"  Std: {embeddings.std():.4f}\")\nprint(f\"  Range: [{embeddings.min():.4f}, {embeddings.max():.4f}]\")\nprint(f\"\\n\u2713 No NaN values: {not np.isnan(embeddings).any()}\")\nprint(f\"\u2713 No Inf values: {not np.isinf(embeddings).any()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 2. Qualitative Evaluation: t-SNE Visualization\n\n**What is t-SNE?** A dimensionality reduction technique that projects high-dimensional embeddings (128-dim) to 2D while preserving local structure.\n\n**What to look for:**\n- \u2705 **Good**: Clear, distinct clusters for different event types\n- \u2705 **Good**: Anomalies appear as scattered outliers\n- \u274c **Bad**: All points in one giant blob (no structure learned)\n- \u274c **Bad**: Random scatter with no clusters\n\n**Perplexity parameter**: Controls balance between local and global structure\n- Low (5-15): Focus on local neighborhoods\n- Medium (30): Default, balanced view\n- High (50): Emphasize global structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample for visualization (t-SNE is slow on large datasets)\nsample_size = min(3000, len(embeddings))\nnp.random.seed(42)\nindices = np.random.choice(len(embeddings), sample_size, replace=False)\nemb_sample = embeddings[indices]\n\nprint(f\"Sampling {sample_size:,} embeddings for t-SNE visualization\")\nprint(f\"(Running t-SNE on full dataset would be too slow)\")\nprint(f\"\\nRunning t-SNE with perplexity=30... (this may take 1-2 minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run t-SNE\ntsne = TSNE(n_components=2, perplexity=30, random_state=42, max_iter=1000)\nemb_2d = tsne.fit_transform(emb_sample)\nprint(\"\u2713 t-SNE complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize t-SNE\nfig, axes = plt.subplots(1, 2, figsize=(15, 6))\n\n# Basic scatter\naxes[0].scatter(emb_2d[:, 0], emb_2d[:, 1], alpha=0.6, s=20, c='steelblue', edgecolors='none')\naxes[0].set_xlabel('t-SNE Dimension 1', fontsize=12)\naxes[0].set_ylabel('t-SNE Dimension 2', fontsize=12)\naxes[0].set_title('OCSF Event Embeddings (t-SNE)', fontsize=14, fontweight='bold')\naxes[0].grid(True, alpha=0.3)\n\n# Colored by embedding norm (potential anomaly indicator)\nnorms = np.linalg.norm(emb_sample, axis=1)\nscatter = axes[1].scatter(emb_2d[:, 0], emb_2d[:, 1], c=norms,\n                          cmap='viridis', alpha=0.6, s=20, edgecolors='none')\naxes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)\naxes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)\naxes[1].set_title('Colored by Embedding Norm (Anomaly Indicator)', fontsize=14, fontweight='bold')\naxes[1].grid(True, alpha=0.3)\ncbar = plt.colorbar(scatter, ax=axes[1])\ncbar.set_label('L2 Norm', fontsize=11)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"INTERPRETATION GUIDE\")\nprint(\"=\"*60)\nprint(\"\u2713 Look for distinct clusters (similar events group together)\")\nprint(\"\u2713 Outliers/sparse regions = potential anomalies\")\nprint(\"\u2713 Right plot: Yellow points (high norm) = unusual events\")\nprint(\"\u2717 Single blob = poor embeddings, need more training\")\nprint(\"\u2717 Random scatter = model didn't learn structure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 3. Quantitative Evaluation: Cluster Quality Metrics\n\nVisualization is subjective - we need objective numbers to:\n- Compare different models\n- Track quality over time\n- Set production deployment thresholds\n\n### Silhouette Score\n\n**What it measures**: How well-separated clusters are (range: -1 to +1, higher is better)\n\n**Interpretation**:\n- **0.7-1.0**: Excellent separation\n- **0.5-0.7**: Reasonable structure (acceptable for production)\n- **0.25-0.5**: Weak structure\n- **< 0.25**: Poor clustering\n\n**Target for production**: > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run k-means clustering to identify natural clusters\nn_clusters = 3  # Try 3-5 clusters for most OCSF data\n\nprint(f\"Running k-means with {n_clusters} clusters...\")\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\ncluster_labels = kmeans.fit_predict(embeddings)\n\n# Compute silhouette score\nsilhouette_avg = silhouette_score(embeddings, cluster_labels)\nsample_silhouette_values = silhouette_samples(embeddings, cluster_labels)\n\nprint(f\"\\n{'='*60}\")\nprint(f\"SILHOUETTE SCORE: {silhouette_avg:.3f}\")\nprint(f\"{'='*60}\")\nprint(f\"\\nInterpretation:\")\nif silhouette_avg > 0.7:\n    print(f\"  \u2713 EXCELLENT - Strong cluster separation\")\nelif silhouette_avg > 0.5:\n    print(f\"  \u2713 GOOD - Acceptable for production\")\nelif silhouette_avg > 0.25:\n    print(f\"  \u26a0 WEAK - May miss subtle anomalies\")\nelse:\n    print(f\"  \u2717 POOR - Embeddings not useful, retrain needed\")\n\nprint(f\"\\nCluster sizes: {np.bincount(cluster_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize silhouette plot\nfig, ax = plt.subplots(figsize=(10, 7))\n\ny_lower = 10\ncolors = plt.cm.tab10(np.linspace(0, 1, n_clusters))\n\nfor i in range(n_clusters):\n    # Get silhouette values for cluster i\n    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n\n    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n                     facecolor=colors[i], edgecolor=colors[i], alpha=0.7)\n\n    # Label cluster\n    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, f\"Cluster {i}\\n(n={size_cluster_i})\")\n    y_lower = y_upper + 10\n\n# Add average silhouette score line\nax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\", linewidth=2,\n          label=f\"Average: {silhouette_avg:.3f}\")\n\n# Add threshold lines\nax.axvline(x=0.5, color=\"green\", linestyle=\":\", linewidth=1.5, alpha=0.7,\n          label=\"Production threshold: 0.5\")\n\nax.set_title(\"Silhouette Plot - Cluster Quality Analysis\", fontsize=14, fontweight='bold')\nax.set_xlabel(\"Silhouette Coefficient\", fontsize=12)\nax.set_ylabel(\"Cluster\", fontsize=12)\nax.legend(loc='best')\nax.grid(True, alpha=0.3, axis='x')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"READING THE SILHOUETTE PLOT\")\nprint(\"=\"*60)\nprint(\"Width of colored bands = silhouette scores for samples in that cluster\")\nprint(\"  - Wide spread \u2192 cluster has outliers or mixed events\")\nprint(\"  - Narrow spread \u2192 cohesive, consistent cluster\")\nprint(\"\\nPoints below zero = probably in wrong cluster\")\nprint(\"Red dashed line (average) should be > 0.5 for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Cluster Quality Metrics\n\n**Davies-Bouldin Index**: Measures cluster overlap (lower is better, min 0)\n- < 1.0: Good separation\n- 1.0-2.0: Moderate separation\n- > 2.0: Poor separation\n\n**Calinski-Harabasz Score**: Ratio of between/within cluster variance (higher is better)\n- No fixed threshold, use for relative comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute additional metrics\ndavies_bouldin = davies_bouldin_score(embeddings, cluster_labels)\ncalinski_harabasz = calinski_harabasz_score(embeddings, cluster_labels)\n\nprint(f\"{'='*60}\")\nprint(f\"COMPREHENSIVE CLUSTER QUALITY METRICS\")\nprint(f\"{'='*60}\")\nprint(f\"\\n{'Metric':<30} {'Value':<12} {'Status'}\")\nprint(f\"{'-'*60}\")\nprint(f\"{'Silhouette Score':<30} {silhouette_avg:<12.3f} {'\u2713 Good' if silhouette_avg > 0.5 else '\u2717 Poor'}\")\nprint(f\"{'Davies-Bouldin Index':<30} {davies_bouldin:<12.3f} {'\u2713 Good' if davies_bouldin < 1.0 else '\u26a0 Moderate'}\")\nprint(f\"{'Calinski-Harabasz Score':<30} {calinski_harabasz:<12.1f} {'(higher=better)'}\")\nprint(f\"{'-'*60}\")\n\n# Overall verdict\npassed = silhouette_avg > 0.5 and davies_bouldin < 1.5\nverdict = \"PASS \u2713\" if passed else \"NEEDS IMPROVEMENT \u26a0\"\nprint(f\"\\nOverall Quality Verdict: {verdict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## 4. Comprehensive Quality Report\n\nGenerate a summary report of all evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quality_report(embeddings, cluster_labels, silhouette_avg,\n                           davies_bouldin, calinski_harabasz):\n    \"\"\"\n    Generate comprehensive embedding quality report.\n    \"\"\"\n    report = {\n        'num_samples': len(embeddings),\n        'embedding_dim': embeddings.shape[1],\n        'num_clusters': len(np.unique(cluster_labels)),\n        'silhouette_score': silhouette_avg,\n        'davies_bouldin_index': davies_bouldin,\n        'calinski_harabasz_score': calinski_harabasz,\n    }\n\n    # Quality verdict\n    passed = report['silhouette_score'] > 0.5 and report['davies_bouldin_index'] < 1.5\n    report['verdict'] = 'PASS' if passed else 'FAIL'\n\n    return report\n\n# Generate report\nreport = generate_quality_report(\n    embeddings, cluster_labels, silhouette_avg,\n    davies_bouldin, calinski_harabasz\n)\n\n# Display report\nprint(\"\\n\" + \"=\"*70)\nprint(\" \"*20 + \"EMBEDDING QUALITY REPORT\")\nprint(\"=\"*70)\nprint(f\"\\nDataset:\")\nprint(f\"  Total samples: {report['num_samples']:,}\")\nprint(f\"  Embedding dimension: {report['embedding_dim']}\")\nprint(f\"  Clusters identified: {report['num_clusters']}\")\n\nprint(f\"\\nCluster Quality Metrics:\")\nprint(f\"  Silhouette Score:        {report['silhouette_score']:.3f}  {'\u2713' if report['silhouette_score'] > 0.5 else '\u2717'}\")\nprint(f\"  Davies-Bouldin Index:    {report['davies_bouldin_index']:.3f}  {'\u2713' if report['davies_bouldin_index'] < 1.0 else '\u26a0'}\")\nprint(f\"  Calinski-Harabasz Score: {report['calinski_harabasz_score']:.1f}\")\n\nprint(f\"\\nProduction Readiness:\")\nif report['silhouette_score'] > 0.5:\n    print(f\"  \u2713 Cluster separation: ACCEPTABLE (> 0.5)\")\nelse:\n    print(f\"  \u2717 Cluster separation: POOR (< 0.5)\")\n\nif report['davies_bouldin_index'] < 1.0:\n    print(f\"  \u2713 Cluster overlap: LOW (< 1.0)\")\nelif report['davies_bouldin_index'] < 1.5:\n    print(f\"  \u26a0 Cluster overlap: MODERATE (1.0-1.5)\")\nelse:\n    print(f\"  \u2717 Cluster overlap: HIGH (> 1.5)\")\n\nprint(f\"\\n{'='*70}\")\nprint(f\"VERDICT: {report['verdict']}\")\nprint(f\"{'='*70}\")\n\nif report['verdict'] == 'PASS':\n    print(\"\\n\u2713 Embeddings are suitable for production anomaly detection\")\n    print(\"  Proceed to notebook 06 (Anomaly Detection)\")\nelse:\n    print(\"\\n\u26a0 Embeddings need improvement:\")\n    print(\"  - Try training for more epochs (notebook 04)\")\n    print(\"  - Check feature engineering (notebook 03)\")\n    print(\"  - Adjust model capacity (d_model, num_blocks)\")\n    print(\"  - Use stronger augmentation during training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n## Summary\n\nIn this notebook, we evaluated embedding quality using:\n\n### Qualitative Evaluation\n1. **t-SNE Visualization** - Projected 128-dim embeddings to 2D\n   - Identified visual clusters and outliers\n   - Colored by embedding norm to spot anomalies\n\n### Quantitative Evaluation\n2. **Cluster Quality Metrics** - Objective numbers\n   - **Silhouette Score**: Measures cluster separation (target > 0.5)\n   - **Davies-Bouldin Index**: Measures cluster overlap (target < 1.0)\n   - **Calinski-Harabasz Score**: Higher is better\n\n3. **Quality Report** - Overall production readiness verdict\n\n**Key Takeaway**: Embeddings must pass both quantitative thresholds AND qualitative inspection before production deployment.\n\n**Next steps:**\n- \u2713 If PASS: Proceed to [06-anomaly-detection.ipynb](06-anomaly-detection.ipynb)\n- \u26a0 If FAIL: Return to [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb) to improve training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}