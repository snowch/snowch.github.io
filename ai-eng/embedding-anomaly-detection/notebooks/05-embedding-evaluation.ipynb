<cell id="cell-0"><cell_type>markdown</cell_type># Appendix: Embedding Evaluation

> **Theory**: See [Part 5: Evaluating Embedding Quality](../part5-embedding-quality.md) for concepts behind embedding evaluation.

Evaluate the quality of your trained embeddings using both quantitative metrics and qualitative inspection.

**What you'll learn:**
1. Visualize embeddings with t-SNE and UMAP
2. Compute cluster quality metrics (Silhouette, Davies-Bouldin)
3. Inspect nearest neighbors to verify semantic similarity
4. Test embedding robustness to input perturbations
5. Generate a comprehensive quality report

**Prerequisites:**
- Embeddings from [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb)
- Trained model (`tabular_resnet.pt`)

---

## Why Evaluate Embeddings?

**The challenge**: Just because your training loss decreased doesn't mean your embeddings are useful. A model can memorize training data while learning poor representations.

**The solution**: Evaluate from multiple angles:
- **Quantitative** (objective metrics like Silhouette Score)
- **Qualitative** (visual inspection, nearest neighbors)

Numbers don't tell the whole story - you need to *look* at your data!</cell id="cell-0">
<cell id="cell-1">import numpy as np
import pickle
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, silhouette_samples
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import cross_val_score

print("✓ All imports successful")
print("\nLibraries loaded:")
print("  - NumPy for numerical operations")
print("  - Matplotlib for visualization")
print("  - Scikit-learn for clustering and metrics")</cell id="cell-1">
<cell id="cell-2"><cell_type>markdown</cell_type>## 1. Load Embeddings

Load the embeddings generated in the self-supervised training notebook.

**What you should expect:**
- Shape: `(N, 128)` - one 128-dimensional vector per OCSF event
- Values roughly centered around 0
- No NaN or Inf values

**If you see errors:**
- `FileNotFoundError`: Run notebook 04 first to generate embeddings
- Wrong shape: Ensure you're using the correct embedding file</cell id="cell-2">
<cell id="cell-3"># Load embeddings
embeddings = np.load('../data/embeddings.npy')

# Load original features (for perturbation testing later)
numerical = np.load('../data/numerical_features.npy')
categorical = np.load('../data/categorical_features.npy')

with open('../data/feature_artifacts.pkl', 'rb') as f:
    artifacts = pickle.load(f)

print("Loaded Embeddings:")
print(f"  Shape: {embeddings.shape}")
print(f"  Mean: {embeddings.mean():.4f}")
print(f"  Std: {embeddings.std():.4f}")
print(f"  Range: [{embeddings.min():.4f}, {embeddings.max():.4f}]")
print(f"\n✓ No NaN values: {not np.isnan(embeddings).any()}")
print(f"✓ No Inf values: {not np.isinf(embeddings).any()}")</cell id="cell-3">
<cell id="cell-4"><cell_type>markdown</cell_type>---

## 2. Qualitative Evaluation: t-SNE Visualization

**What is t-SNE?** A dimensionality reduction technique that projects high-dimensional embeddings (128-dim) to 2D while preserving local structure.

**What to look for:**
- ✅ **Good**: Clear, distinct clusters for different event types
- ✅ **Good**: Anomalies appear as scattered outliers
- ❌ **Bad**: All points in one giant blob (no structure learned)
- ❌ **Bad**: Random scatter with no clusters

**Perplexity parameter**: Controls balance between local and global structure
- Low (5-15): Focus on local neighborhoods
- Medium (30): Default, balanced view
- High (50): Emphasize global structure</cell id="cell-4">
<cell id="cell-5"># Sample for visualization (t-SNE is slow on large datasets)
sample_size = min(3000, len(embeddings))
np.random.seed(42)
indices = np.random.choice(len(embeddings), sample_size, replace=False)
emb_sample = embeddings[indices]

print(f"Sampling {sample_size:,} embeddings for t-SNE visualization")
print(f"(Running t-SNE on full dataset would be too slow)")
print(f"\nRunning t-SNE with perplexity=30... (this may take 1-2 minutes)")</cell id="cell-5">
<cell id="cell-6"># Run t-SNE
tsne = TSNE(n_components=2, perplexity=30, random_state=42, max_iter=1000)
emb_2d = tsne.fit_transform(emb_sample)
print("✓ t-SNE complete!")</cell id="cell-6">
<cell id="cell-7"># Visualize t-SNE
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# Basic scatter
axes[0].scatter(emb_2d[:, 0], emb_2d[:, 1], alpha=0.6, s=20, c='steelblue', edgecolors='none')
axes[0].set_xlabel('t-SNE Dimension 1', fontsize=12)
axes[0].set_ylabel('t-SNE Dimension 2', fontsize=12)
axes[0].set_title('OCSF Event Embeddings (t-SNE)', fontsize=14, fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Colored by embedding norm (potential anomaly indicator)
norms = np.linalg.norm(emb_sample, axis=1)
scatter = axes[1].scatter(emb_2d[:, 0], emb_2d[:, 1], c=norms,
                          cmap='viridis', alpha=0.6, s=20, edgecolors='none')
axes[1].set_xlabel('t-SNE Dimension 1', fontsize=12)
axes[1].set_ylabel('t-SNE Dimension 2', fontsize=12)
axes[1].set_title('Colored by Embedding Norm (Anomaly Indicator)', fontsize=14, fontweight='bold')
axes[1].grid(True, alpha=0.3)
cbar = plt.colorbar(scatter, ax=axes[1])
cbar.set_label('L2 Norm', fontsize=11)

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("INTERPRETATION GUIDE")
print("="*60)
print("✓ Look for distinct clusters (similar events group together)")
print("✓ Outliers/sparse regions = potential anomalies")
print("✓ Right plot: Yellow points (high norm) = unusual events")
print("✗ Single blob = poor embeddings, need more training")
print("✗ Random scatter = model didn't learn structure")</cell id="cell-7">
<cell id="cell-8"><cell_type>markdown</cell_type>### How to read the t-SNE plots

**Left plot (Basic scatter)**:
- **Clusters**: Groups of points represent similar events (e.g., same activity type, status)
- **Separation**: Space between clusters indicates model learned to distinguish event types
- **Outliers**: Isolated points far from clusters = potential anomalies

**Right plot (Colored by norm)**:
- **Yellow/bright points**: High embedding norm = events far from "typical" center
- **Purple/dark points**: Low norm = common, normal events
- **Pattern**: Anomalies often have higher norms (stand out from the crowd)

**Red flags**:
- All points overlapping in one region → model didn't learn useful features
- No visual clusters → try more training epochs or check feature engineering</cell id="cell-8">
<cell id="cell-9"><cell_type>markdown</cell_type>---

## 3. Quantitative Evaluation: Cluster Quality Metrics

Visualization is subjective - we need objective numbers to:
- Compare different models
- Track quality over time
- Set production deployment thresholds

### Silhouette Score

**What it measures**: How well-separated clusters are (range: -1 to +1, higher is better)

**Interpretation**:
- **0.7-1.0**: Excellent separation
- **0.5-0.7**: Reasonable structure (acceptable for production)
- **0.25-0.5**: Weak structure
- **< 0.25**: Poor clustering

**Target for production**: > 0.5</cell id="cell-9">
<cell id="cell-10"># Run k-means clustering to identify natural clusters
n_clusters = 3  # Try 3-5 clusters for most OCSF data

print(f"Running k-means with {n_clusters} clusters...")
kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
cluster_labels = kmeans.fit_predict(embeddings)

# Compute silhouette score
silhouette_avg = silhouette_score(embeddings, cluster_labels)
sample_silhouette_values = silhouette_samples(embeddings, cluster_labels)

print(f"\n{'='*60}")
print(f"SILHOUETTE SCORE: {silhouette_avg:.3f}")
print(f"{'='*60}")
print(f"\nInterpretation:")
if silhouette_avg > 0.7:
    print(f"  ✓ EXCELLENT - Strong cluster separation")
elif silhouette_avg > 0.5:
    print(f"  ✓ GOOD - Acceptable for production")
elif silhouette_avg > 0.25:
    print(f"  ⚠ WEAK - May miss subtle anomalies")
else:
    print(f"  ✗ POOR - Embeddings not useful, retrain needed")

print(f"\nCluster sizes: {np.bincount(cluster_labels)}")</cell id="cell-10">
<cell id="cell-11"># Visualize silhouette plot
fig, ax = plt.subplots(figsize=(10, 7))

y_lower = 10
colors = plt.cm.tab10(np.linspace(0, 1, n_clusters))

for i in range(n_clusters):
    # Get silhouette values for cluster i
    ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
    ith_cluster_silhouette_values.sort()

    size_cluster_i = ith_cluster_silhouette_values.shape[0]
    y_upper = y_lower + size_cluster_i

    ax.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,
                     facecolor=colors[i], edgecolor=colors[i], alpha=0.7)

    # Label cluster
    ax.text(-0.05, y_lower + 0.5 * size_cluster_i, f"Cluster {i}\n(n={size_cluster_i})")
    y_lower = y_upper + 10

# Add average silhouette score line
ax.axvline(x=silhouette_avg, color="red", linestyle="--", linewidth=2,
          label=f"Average: {silhouette_avg:.3f}")

# Add threshold lines
ax.axvline(x=0.5, color="green", linestyle=":", linewidth=1.5, alpha=0.7,
          label="Production threshold: 0.5")

ax.set_title("Silhouette Plot - Cluster Quality Analysis", fontsize=14, fontweight='bold')
ax.set_xlabel("Silhouette Coefficient", fontsize=12)
ax.set_ylabel("Cluster", fontsize=12)
ax.legend(loc='best')
ax.grid(True, alpha=0.3, axis='x')

plt.tight_layout()
plt.show()

print("\n" + "="*60)
print("READING THE SILHOUETTE PLOT")
print("="*60)
print("Width of colored bands = silhouette scores for samples in that cluster")
print("  - Wide spread → cluster has outliers or mixed events")
print("  - Narrow spread → cohesive, consistent cluster")
print("\nPoints below zero = probably in wrong cluster")
print("Red dashed line (average) should be > 0.5 for production")</cell id="cell-11">
<cell id="cell-12"><cell_type>markdown</cell_type>### Other Cluster Quality Metrics

**Davies-Bouldin Index**: Measures cluster overlap (lower is better, min 0)
- < 1.0: Good separation
- 1.0-2.0: Moderate separation
- > 2.0: Poor separation

**Calinski-Harabasz Score**: Ratio of between/within cluster variance (higher is better)
- No fixed threshold, use for relative comparison</cell id="cell-12">
<cell id="cell-13"># Compute additional metrics
davies_bouldin = davies_bouldin_score(embeddings, cluster_labels)
calinski_harabasz = calinski_harabasz_score(embeddings, cluster_labels)

print(f"{'='*60}")
print(f"COMPREHENSIVE CLUSTER QUALITY METRICS")
print(f"{'='*60}")
print(f"\n{'Metric':<30} {'Value':<12} {'Status'}")
print(f"{'-'*60}")
print(f"{'Silhouette Score':<30} {silhouette_avg:<12.3f} {'✓ Good' if silhouette_avg > 0.5 else '✗ Poor'}")
print(f"{'Davies-Bouldin Index':<30} {davies_bouldin:<12.3f} {'✓ Good' if davies_bouldin < 1.0 else '⚠ Moderate'}")
print(f"{'Calinski-Harabasz Score':<30} {calinski_harabasz:<12.1f} {'(higher=better)'}")
print(f"{'-'*60}")

# Overall verdict
passed = silhouette_avg > 0.5 and davies_bouldin < 1.5
verdict = "PASS ✓" if passed else "NEEDS IMPROVEMENT ⚠"
print(f"\nOverall Quality Verdict: {verdict}")</cell id="cell-13">
<cell id="cell-14"><cell_type>markdown</cell_type>### Comparing Different Numbers of Clusters

Let's find the optimal number of clusters by comparing metrics across different k values.</cell id="cell-14">
<cell id="cell-15"># Try different numbers of clusters
k_range = range(2, 8)
results = []

print("Testing different cluster counts...")
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(embeddings)

    sil = silhouette_score(embeddings, labels)
    db = davies_bouldin_score(embeddings, labels)
    ch = calinski_harabasz_score(embeddings, labels)

    results.append({
        'k': k,
        'silhouette': sil,
        'davies_bouldin': db,
        'calinski_harabasz': ch
    })

# Display results
print(f"\n{'='*70}")
print(f"OPTIMAL CLUSTER COUNT ANALYSIS")
print(f"{'='*70}")
print(f"{'K':<5} {'Silhouette':<15} {'Davies-Bouldin':<18} {'Calinski-Harabasz':<18}")
print(f"{'-'*70}")
for r in results:
    print(f"{r['k']:<5} {r['silhouette']:<15.3f} {r['davies_bouldin']:<18.3f} {r['calinski_harabasz']:<18.1f}")

# Find best k
best_k = max(results, key=lambda x: x['silhouette'])['k']
print(f"\nRecommended k: {best_k} (highest Silhouette Score)")</cell id="cell-15">
<cell id="cell-16"># Visualize metrics across k
fig, axes = plt.subplots(1, 3, figsize=(16, 5))

k_values = [r['k'] for r in results]

# Silhouette
axes[0].plot(k_values, [r['silhouette'] for r in results], 'o-', linewidth=2, markersize=8)
axes[0].axhline(0.5, color='green', linestyle='--', alpha=0.7, label='Production threshold')
axes[0].set_xlabel('Number of Clusters (k)', fontsize=11)
axes[0].set_ylabel('Silhouette Score', fontsize=11)
axes[0].set_title('Silhouette Score vs k\n(higher is better)', fontsize=12, fontweight='bold')
axes[0].grid(True, alpha=0.3)
axes[0].legend()

# Davies-Bouldin
axes[1].plot(k_values, [r['davies_bouldin'] for r in results], 'o-', linewidth=2, markersize=8, color='orange')
axes[1].axhline(1.0, color='green', linestyle='--', alpha=0.7, label='Good threshold')
axes[1].set_xlabel('Number of Clusters (k)', fontsize=11)
axes[1].set_ylabel('Davies-Bouldin Index', fontsize=11)
axes[1].set_title('Davies-Bouldin Index vs k\n(lower is better)', fontsize=12, fontweight='bold')
axes[1].grid(True, alpha=0.3)
axes[1].legend()

# Calinski-Harabasz
axes[2].plot(k_values, [r['calinski_harabasz'] for r in results], 'o-', linewidth=2, markersize=8, color='green')
axes[2].set_xlabel('Number of Clusters (k)', fontsize=11)
axes[2].set_ylabel('Calinski-Harabasz Score', fontsize=11)
axes[2].set_title('Calinski-Harabasz Score vs k\n(higher is better)', fontsize=12, fontweight='bold')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\nLook for 'elbow' points where improvement slows down")</cell id="cell-16">
<cell id="cell-17"><cell_type>markdown</cell_type>---

## 4. Qualitative Evaluation: Nearest Neighbor Inspection

**Why this matters**: Cluster metrics show overall structure, but you need to verify individual embeddings make sense.

**The test**: Pick a sample event, find its k nearest neighbors, and check if they're semantically similar.

**What to look for**:
- ✅ Neighbors are actually similar events (same activity, status, user patterns)
- ✗ Neighbors are random unrelated events
- ✗ Model confuses critical distinctions (e.g., success vs failure)</cell id="cell-17">
<cell id="cell-18">def find_nearest_neighbors(query_idx, embeddings, k=10):
    """
    Find k nearest neighbors for a query embedding.

    Returns:
        indices and cosine similarities of nearest neighbors
    """
    # Normalize embeddings for cosine similarity
    query_norm = embeddings[query_idx] / np.linalg.norm(embeddings[query_idx])
    all_norms = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

    # Compute cosine similarities
    similarities = np.dot(all_norms, query_norm)

    # Find top-k (excluding query itself)
    top_k_indices = np.argsort(similarities)[::-1][1:k+1]

    return top_k_indices, similarities[top_k_indices]

# Inspect a random sample
np.random.seed(42)
query_idx = np.random.randint(0, len(embeddings))

neighbors, sims = find_nearest_neighbors(query_idx, embeddings, k=10)

print(f"{'='*60}")
print(f"NEAREST NEIGHBOR INSPECTION")
print(f"{'='*60}")
print(f"\nQuery Event Index: {query_idx}")
print(f"Query Embedding Norm: {np.linalg.norm(embeddings[query_idx]):.3f}")
print(f"\nTop 10 Nearest Neighbors:")
print(f"{'-'*60}")
print(f"{'Rank':<6} {'Index':<10} {'Cosine Similarity':<20} {'Status'}")
print(f"{'-'*60}")

for rank, (idx, sim) in enumerate(zip(neighbors, sims), 1):
    if sim > 0.95:
        status = "Very similar ✓"
    elif sim > 0.85:
        status = "Similar ✓"
    elif sim > 0.70:
        status = "Somewhat similar"
    else:
        status = "Different ⚠"

    print(f"{rank:<6} {idx:<10} {sim:<20.4f} {status}")

print(f"\n{'='*60}")
print(f"INTERPRETATION")
print(f"{'='*60}")
print(f"✓ Good: High similarities (> 0.85) indicate neighbors are truly similar")
print(f"✓ Good: Gradual decline in similarity (not sudden drops)")
print(f"✗ Bad: Low similarities (< 0.70) mean neighbors aren't actually similar")
print(f"✗ Bad: Random similarity values mean poor embedding quality")</cell id="cell-18">
<cell id="cell-19"># Visualize nearest neighbors in t-SNE space
if len(emb_sample) > query_idx:
    # Check if query is in our sampled data
    sample_map = {orig_idx: sample_idx for sample_idx, orig_idx in enumerate(indices)}

    if query_idx in sample_map and all(n in sample_map for n in neighbors[:5]):
        fig, ax = plt.subplots(figsize=(10, 8))

        # Plot all points
        ax.scatter(emb_2d[:, 0], emb_2d[:, 1], alpha=0.3, s=20, c='lightgray', label='Other events')

        # Highlight query
        query_2d_idx = sample_map[query_idx]
        ax.scatter(emb_2d[query_2d_idx, 0], emb_2d[query_2d_idx, 1],
                  s=200, c='red', marker='*', edgecolors='black', linewidth=2,
                  label='Query event', zorder=5)

        # Highlight neighbors (top 5 in sample)
        neighbor_2d_indices = [sample_map[n] for n in neighbors[:5] if n in sample_map]
        if neighbor_2d_indices:
            ax.scatter(emb_2d[neighbor_2d_indices, 0], emb_2d[neighbor_2d_indices, 1],
                      s=100, c='green', marker='o', edgecolors='black', linewidth=1.5,
                      label='Top 5 neighbors', zorder=4)

            # Draw lines to neighbors
            for n_idx in neighbor_2d_indices:
                ax.plot([emb_2d[query_2d_idx, 0], emb_2d[n_idx, 0]],
                       [emb_2d[query_2d_idx, 1], emb_2d[n_idx, 1]],
                       'g--', alpha=0.5, linewidth=1)

        ax.set_xlabel('t-SNE Dimension 1', fontsize=12)
        ax.set_ylabel('t-SNE Dimension 2', fontsize=12)
        ax.set_title('Nearest Neighbors in Embedding Space', fontsize=14, fontweight='bold')
        ax.legend(loc='best')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        print("\n✓ Visualization shows query event (red star) and its nearest neighbors (green circles)")
        print("  Good embeddings: neighbors should be close in 2D space")
    else:
        print("\n(Query or neighbors not in t-SNE sample, skipping visualization)")
else:
    print("\n(Sample size too small for visualization)")</cell id="cell-19">
<cell id="cell-20"><cell_type>markdown</cell_type>---

## 5. Embedding Robustness Test

**Why this matters**: In production, your OCSF data will have noise (network jitter, rounding errors). Good embeddings should be stable under small perturbations.

**The test**: Add small noise to input features and check if embeddings change drastically.

**Interpretation**:
- **> 0.95**: Very stable (robust to noise)
- **0.85-0.95**: Moderately stable (acceptable)
- **< 0.85**: Unstable (fragile embeddings, need more regularization)

**Note**: This requires the trained model. If you don't have it loaded, skip this section.</cell id="cell-20">
<cell id="cell-21"># Try to load the model for robustness testing
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F

    # Check if we have the model architecture definitions
    # (You may need to copy the model classes from notebook 04)

    print("Robustness testing requires the trained model.")
    print("If you have the model architecture defined, uncomment and run the code below.")
    print("\n(Skipping robustness test in this notebook - see Part 5 text for implementation)")

except ImportError:
    print("PyTorch not available - skipping robustness test")
    print("Install with: pip install torch")</cell id="cell-21">
<cell id="cell-22"><cell_type>markdown</cell_type>---

## 6. Comprehensive Quality Report

Generate a summary report of all evaluation metrics.</cell id="cell-22">
<cell id="cell-23">def generate_quality_report(embeddings, cluster_labels, silhouette_avg,
                           davies_bouldin, calinski_harabasz):
    """
    Generate comprehensive embedding quality report.
    """
    report = {
        'num_samples': len(embeddings),
        'embedding_dim': embeddings.shape[1],
        'num_clusters': len(np.unique(cluster_labels)),
        'silhouette_score': silhouette_avg,
        'davies_bouldin_index': davies_bouldin,
        'calinski_harabasz_score': calinski_harabasz,
    }

    # Quality verdict
    passed = report['silhouette_score'] > 0.5 and report['davies_bouldin_index'] < 1.5
    report['verdict'] = 'PASS' if passed else 'FAIL'

    return report

# Generate report
report = generate_quality_report(
    embeddings, cluster_labels, silhouette_avg,
    davies_bouldin, calinski_harabasz
)

# Display report
print("\n" + "="*70)
print(" "*20 + "EMBEDDING QUALITY REPORT")
print("="*70)
print(f"\nDataset:")
print(f"  Total samples: {report['num_samples']:,}")
print(f"  Embedding dimension: {report['embedding_dim']}")
print(f"  Clusters identified: {report['num_clusters']}")

print(f"\nCluster Quality Metrics:")
print(f"  Silhouette Score:        {report['silhouette_score']:.3f}  {'✓' if report['silhouette_score'] > 0.5 else '✗'}")
print(f"  Davies-Bouldin Index:    {report['davies_bouldin_index']:.3f}  {'✓' if report['davies_bouldin_index'] < 1.0 else '⚠'}")
print(f"  Calinski-Harabasz Score: {report['calinski_harabasz_score']:.1f}")

print(f"\nProduction Readiness:")
if report['silhouette_score'] > 0.5:
    print(f"  ✓ Cluster separation: ACCEPTABLE (> 0.5)")
else:
    print(f"  ✗ Cluster separation: POOR (< 0.5)")

if report['davies_bouldin_index'] < 1.0:
    print(f"  ✓ Cluster overlap: LOW (< 1.0)")
elif report['davies_bouldin_index'] < 1.5:
    print(f"  ⚠ Cluster overlap: MODERATE (1.0-1.5)")
else:
    print(f"  ✗ Cluster overlap: HIGH (> 1.5)")

print(f"\n{'='*70}")
print(f"VERDICT: {report['verdict']}")
print(f"{'='*70}")

if report['verdict'] == 'PASS':
    print("\n✓ Embeddings are suitable for production anomaly detection")
    print("  Proceed to notebook 06 (Anomaly Detection)")
else:
    print("\n⚠ Embeddings need improvement:")
    print("  - Try training for more epochs (notebook 04)")
    print("  - Check feature engineering (notebook 03)")
    print("  - Adjust model capacity (d_model, num_blocks)")
    print("  - Use stronger augmentation during training")</cell id="cell-23">
<cell id="cell-24"><cell_type>markdown</cell_type>---

## Summary

In this notebook, we evaluated embedding quality using:

### Qualitative Evaluation
1. **t-SNE Visualization** - Projected 128-dim embeddings to 2D
   - Identified visual clusters and outliers
   - Colored by embedding norm to spot anomalies

2. **Nearest Neighbor Inspection** - Verified semantic similarity
   - Found k-nearest neighbors for sample events
   - Checked if neighbors are truly similar (high cosine similarity)

### Quantitative Evaluation
3. **Cluster Quality Metrics** - Objective numbers
   - **Silhouette Score**: {silhouette_avg:.3f} (target > 0.5)
   - **Davies-Bouldin Index**: {davies_bouldin:.3f} (target < 1.0)
   - **Calinski-Harabasz Score**: {calinski_harabasz:.1f} (higher is better)

4. **Optimal Cluster Count** - Compared k=2 to k=7
   - Found best k based on multiple metrics
   - Visualized metric trends

5. **Quality Report** - Overall verdict: **{report['verdict']}**

**Key Takeaway**: Embeddings must pass both quantitative thresholds AND qualitative inspection before production deployment.

**Next steps:**
- ✓ If PASS: Proceed to [06-anomaly-detection.ipynb](06-anomaly-detection.ipynb)
- ⚠ If FAIL: Return to [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb) to improve training</cell id="cell-24">