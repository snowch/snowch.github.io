{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Appendix: Feature Engineering for OCSF Data\n\nThis notebook demonstrates feature engineering techniques for OCSF (Open Cybersecurity Schema Framework) data.\n\n**What you'll learn:**\n1. Loading and exploring OCSF parquet data\n2. Understanding the schema and available fields\n3. Engineering temporal features\n4. Handling categorical and numerical features\n5. Preparing data for TabularResNet\n\n**Prerequisites:**\n- Sample data in `../data/` (included in repository)\n- Or generate your own using `../appendix-code/`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load OCSF Data\n",
    "\n",
    "The sample data is already in OCSF-compliant parquet format with flattened fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OCSF logs\n",
    "df = pd.read_parquet('../data/ocsf_logs.parquet')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "print(f\"\\nColumn names:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore OCSF Schema\n",
    "\n",
    "OCSF events have:\n",
    "- **Core fields**: class_uid, category_uid, activity_id, severity_id, time\n",
    "- **Nested objects**: actor, src_endpoint, dst_endpoint, http_request, http_response\n",
    "- **Flattened fields**: actor_user_name, http_request_method, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and non-null counts\n",
    "print(\"Data types and non-null counts:\")\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\nSample of each column type:\")\n",
    "for dtype in df.dtypes.unique():\n",
    "    cols = df.select_dtypes(include=[dtype]).columns[:3].tolist()\n",
    "    print(f\"  {dtype}: {cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values for key categorical columns\n",
    "categorical_cols = ['class_name', 'category_name', 'activity_name', 'status', 'level', 'service']\n",
    "\n",
    "print(\"Unique values in categorical columns:\")\n",
    "for col in categorical_cols:\n",
    "    if col in df.columns:\n",
    "        unique_vals = df[col].nunique()\n",
    "        sample_vals = df[col].value_counts().head(5).index.tolist()\n",
    "        print(f\"\\n{col} ({unique_vals} unique):\")\n",
    "        for val in sample_vals:\n",
    "            count = (df[col] == val).sum()\n",
    "            print(f\"  - {val}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Engineer Temporal Features\n",
    "\n",
    "Time-based patterns are critical for anomaly detection:\n",
    "- Logins at 3 AM are suspicious\n",
    "- Attack patterns have timing signatures\n",
    "- Business hours vs off-hours traffic differs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df, time_col='time'):\n",
    "    \"\"\"\n",
    "    Extract temporal features from Unix timestamp (milliseconds).\n",
    "    \n",
    "    Returns DataFrame with new temporal columns.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Convert milliseconds to datetime\n",
    "    result['datetime'] = pd.to_datetime(result[time_col], unit='ms', errors='coerce')\n",
    "    \n",
    "    # Basic temporal features\n",
    "    result['hour_of_day'] = result['datetime'].dt.hour\n",
    "    result['day_of_week'] = result['datetime'].dt.dayofweek  # 0=Monday\n",
    "    result['is_weekend'] = (result['day_of_week'] >= 5).astype(int)\n",
    "    result['is_business_hours'] = ((result['hour_of_day'] >= 9) & \n",
    "                                    (result['hour_of_day'] < 17)).astype(int)\n",
    "    \n",
    "    # Cyclical encoding (sin/cos) - preserves circular nature\n",
    "    # Hour 23 and hour 0 are close in (sin, cos) space\n",
    "    result['hour_sin'] = np.sin(2 * np.pi * result['hour_of_day'] / 24)\n",
    "    result['hour_cos'] = np.cos(2 * np.pi * result['hour_of_day'] / 24)\n",
    "    result['day_sin'] = np.sin(2 * np.pi * result['day_of_week'] / 7)\n",
    "    result['day_cos'] = np.cos(2 * np.pi * result['day_of_week'] / 7)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply temporal feature extraction\n",
    "df = extract_temporal_features(df)\n",
    "\n",
    "# Show sample of temporal features\n",
    "temporal_cols = ['datetime', 'hour_of_day', 'day_of_week', 'is_weekend', \n",
    "                 'is_business_hours', 'hour_sin', 'hour_cos']\n",
    "df[temporal_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hour distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Hour distribution\n",
    "df['hour_of_day'].hist(bins=24, ax=axes[0], edgecolor='black')\n",
    "axes[0].set_xlabel('Hour of Day')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Event Distribution by Hour')\n",
    "\n",
    "# Day of week distribution\n",
    "df['day_of_week'].hist(bins=7, ax=axes[1], edgecolor='black')\n",
    "axes[1].set_xlabel('Day of Week (0=Mon)')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_title('Event Distribution by Day')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Select Core Features\n",
    "\n",
    "Not all 60+ columns are useful. We select:\n",
    "- **Categorical**: class, activity, status, user, HTTP method\n",
    "- **Numerical**: severity, duration, response codes, temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature sets\n",
    "categorical_features = [\n",
    "    'class_name',\n",
    "    'activity_name', \n",
    "    'status',\n",
    "    'level',\n",
    "    'service',\n",
    "    'actor_user_name',\n",
    "    'http_request_method',\n",
    "    'http_request_url_path',\n",
    "]\n",
    "\n",
    "numerical_features = [\n",
    "    'severity_id',\n",
    "    'activity_id',\n",
    "    'status_id',\n",
    "    'duration',\n",
    "    'http_response_code',\n",
    "    'hour_of_day',\n",
    "    'day_of_week',\n",
    "    'is_weekend',\n",
    "    'is_business_hours',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'day_sin',\n",
    "    'day_cos',\n",
    "]\n",
    "\n",
    "# Filter to columns that exist in our data\n",
    "categorical_features = [c for c in categorical_features if c in df.columns]\n",
    "numerical_features = [c for c in numerical_features if c in df.columns]\n",
    "\n",
    "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"\\nNumerical features ({len(numerical_features)}): {numerical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Missing Values\n",
    "\n",
    "OCSF events have optional fields. Strategy:\n",
    "- **Categorical**: Use special 'MISSING' category\n",
    "- **Numerical**: Use 0 or median, optionally add `_is_missing` indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_values(df, categorical_cols, numerical_cols):\n",
    "    \"\"\"\n",
    "    Handle missing values in feature columns.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Categorical: fill with 'MISSING'\n",
    "    for col in categorical_cols:\n",
    "        if col in result.columns:\n",
    "            result[col] = result[col].fillna('MISSING').astype(str)\n",
    "            result[col] = result[col].replace('', 'MISSING')\n",
    "    \n",
    "    # Numerical: fill with 0\n",
    "    for col in numerical_cols:\n",
    "        if col in result.columns:\n",
    "            result[col] = pd.to_numeric(result[col], errors='coerce').fillna(0)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply missing value handling\n",
    "df_clean = handle_missing_values(df, categorical_features, numerical_features)\n",
    "\n",
    "# Check for remaining nulls\n",
    "all_features = categorical_features + numerical_features\n",
    "null_counts = df_clean[all_features].isnull().sum()\n",
    "print(\"Null counts after handling:\")\n",
    "print(null_counts[null_counts > 0] if null_counts.sum() > 0 else \"No nulls remaining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Features for TabularResNet\n",
    "\n",
    "TabularResNet needs:\n",
    "- **Numerical array**: Normalized floats (mean=0, std=1)\n",
    "- **Categorical array**: Integer indices (0, 1, 2, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def prepare_for_tabular_resnet(df, categorical_cols, numerical_cols):\n",
    "    \"\"\"\n",
    "    Prepare features for TabularResNet.\n",
    "    \n",
    "    Returns:\n",
    "        numerical_array: Normalized numerical features\n",
    "        categorical_array: Integer-encoded categorical features\n",
    "        encoders: Dict of LabelEncoders\n",
    "        scaler: StandardScaler\n",
    "        cardinalities: List of vocab sizes per categorical\n",
    "    \"\"\"\n",
    "    # Encode categorical features\n",
    "    encoders = {}\n",
    "    categorical_data = []\n",
    "    cardinalities = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        # Add 'UNKNOWN' for handling new values at inference\n",
    "        unique_vals = list(df[col].unique()) + ['UNKNOWN']\n",
    "        encoder.fit(unique_vals)\n",
    "        encoded = encoder.transform(df[col])\n",
    "        categorical_data.append(encoded)\n",
    "        encoders[col] = encoder\n",
    "        cardinalities.append(len(encoder.classes_))\n",
    "    \n",
    "    categorical_array = np.column_stack(categorical_data) if categorical_data else np.array([])\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_array = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return numerical_array, categorical_array, encoders, scaler, cardinalities\n",
    "\n",
    "# Prepare features\n",
    "numerical_array, categorical_array, encoders, scaler, cardinalities = \\\n",
    "    prepare_for_tabular_resnet(df_clean, categorical_features, numerical_features)\n",
    "\n",
    "print(\"Feature arrays ready for TabularResNet:\")\n",
    "print(f\"  Numerical shape: {numerical_array.shape}\")\n",
    "print(f\"  Categorical shape: {categorical_array.shape}\")\n",
    "print(f\"  Categorical cardinalities: {cardinalities}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview encoded data\n",
    "print(\"\\nNumerical features (first 3 rows, normalized):\")\n",
    "print(pd.DataFrame(numerical_array[:3], columns=numerical_features).round(3))\n",
    "\n",
    "print(\"\\nCategorical features (first 3 rows, integer encoded):\")\n",
    "print(pd.DataFrame(categorical_array[:3], columns=categorical_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Processed Features\n",
    "\n",
    "Save the processed data and encoding artifacts for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save feature arrays\n",
    "np.save('../data/numerical_features.npy', numerical_array)\n",
    "np.save('../data/categorical_features.npy', categorical_array)\n",
    "\n",
    "# Save encoders and scaler\n",
    "artifacts = {\n",
    "    'encoders': encoders,\n",
    "    'scaler': scaler,\n",
    "    'categorical_cols': categorical_features,\n",
    "    'numerical_cols': numerical_features,\n",
    "    'cardinalities': cardinalities\n",
    "}\n",
    "\n",
    "with open('../data/feature_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  - ../data/numerical_features.npy\")\n",
    "print(\"  - ../data/categorical_features.npy\")\n",
    "print(\"  - ../data/feature_artifacts.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Loaded OCSF data** from parquet format\n",
    "2. **Explored the schema** - 60 columns with nested objects flattened\n",
    "3. **Engineered temporal features** - hour, day, cyclical sin/cos encoding\n",
    "4. **Selected core features** - categorical and numerical subsets\n",
    "5. **Handled missing values** - 'MISSING' for categorical, 0 for numerical\n",
    "6. **Encoded for TabularResNet** - LabelEncoder + StandardScaler\n",
    "\n",
    "**Next**: Use these features in [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb) to train embeddings."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}