{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Feature Engineering for OCSF Data\n",
    "\n",
    "This notebook demonstrates feature engineering techniques for OCSF (Open Cybersecurity Schema Framework) data.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Loading and exploring OCSF parquet data\n",
    "2. Understanding the schema and available fields\n",
    "3. Engineering temporal features\n",
    "4. Handling categorical and numerical features\n",
    "5. Preparing data for TabularResNet\n",
    "\n",
    "**Prerequisites:**\n",
    "- Sample data in `../data/` (included in repository)\n",
    "- Or generate your own using `../appendix-code/`\n",
    "\n",
    "---\n",
    "\n",
    "## Why OCSF?\n",
    "\n",
    "**Without OCSF**, you would need separate models for each log format:\n",
    "- AWS CloudTrail: `eventSource`, `eventName`, `userIdentity.arn`\n",
    "- Okta: `actor.displayName`, `outcome.result`, `target[].type`\n",
    "- Linux auditd: `syscall`, `exe`, `auid`, `comm`\n",
    "\n",
    "**With OCSF**, all sources map to the same schema (`class_uid`, `activity_id`, `actor.user.name`), enabling **one embedding model** to work across all OCSF-compliant sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings for better DataFrame rendering\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load OCSF Data\n",
    "\n",
    "The sample data is in OCSF-compliant parquet format with flattened nested fields.\n",
    "\n",
    "**What you should expect:**\n",
    "- Several thousand events (our sample has ~3000-10000 depending on generation time)\n",
    "- 60+ columns representing OCSF fields\n",
    "- Mix of numerical IDs and string descriptions\n",
    "\n",
    "**If you see errors:**\n",
    "- `FileNotFoundError`: Ensure `../data/ocsf_logs.parquet` exists. Download from the appendix or generate using `../appendix-code/`\n",
    "- `ImportError` for pyarrow: Run `pip install pyarrow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OCSF logs\n",
    "df = pd.read_parquet('../data/ocsf_logs.parquet')\n",
    "\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"  Total events: {len(df):,}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the data - much cleaner than print()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore OCSF Schema\n",
    "\n",
    "OCSF events have a hierarchical structure that has been flattened:\n",
    "- **Core fields**: `class_uid`, `category_uid`, `activity_id`, `severity_id`, `time`\n",
    "- **Nested objects**: `actor`, `src_endpoint`, `dst_endpoint`, `http_request`, `http_response`\n",
    "- **Flattened fields**: `actor_user_name`, `http_request_method`, etc.\n",
    "\n",
    "**What you should expect:**\n",
    "- Mix of `int64`, `float64`, and `object` (string) columns\n",
    "- Many columns may be sparse (lots of nulls) since OCSF fields are optional\n",
    "- Column names follow OCSF naming conventions with underscores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data type distribution\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "print(\"Column types:\")\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"  {dtype}: {count} columns\")\n",
    "\n",
    "# Visualize column types\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "dtype_counts.plot(kind='barh', ax=ax, color='steelblue', edgecolor='black')\n",
    "ax.set_xlabel('Number of Columns')\n",
    "ax.set_ylabel('Data Type')\n",
    "ax.set_title('OCSF Column Types')\n",
    "for i, v in enumerate(dtype_counts):\n",
    "    ax.text(v + 0.5, i, str(v), va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key categorical columns and their distributions\n",
    "categorical_cols = ['class_name', 'category_name', 'activity_name', 'status', 'level', 'service']\n",
    "categorical_cols = [c for c in categorical_cols if c in df.columns]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(categorical_cols[:6]):\n",
    "    value_counts = df[col].value_counts().head(10)\n",
    "    value_counts.plot(kind='barh', ax=axes[i], color='steelblue', edgecolor='black')\n",
    "    axes[i].set_title(f'{col} ({df[col].nunique()} unique)')\n",
    "    axes[i].set_xlabel('Count')\n",
    "\n",
    "plt.suptitle('Key Categorical Column Distributions', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Engineer Temporal Features\n",
    "\n",
    "Time-based patterns are **critical** for anomaly detection:\n",
    "- Logins at 3 AM are suspicious\n",
    "- Attack patterns have timing signatures\n",
    "- Business hours vs off-hours traffic differs significantly\n",
    "\n",
    "**What you should expect:**\n",
    "- `hour_of_day`: 0-23 (midnight = 0)\n",
    "- `day_of_week`: 0-6 (Monday = 0, Sunday = 6)\n",
    "- `hour_sin`/`hour_cos`: Values between -1 and 1 (cyclical encoding)\n",
    "\n",
    "**Why cyclical encoding?**\n",
    "Without it, hour 23 and hour 0 appear far apart (23 vs 0), but they're actually adjacent times. Sin/cos encoding preserves this circular relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_temporal_features(df, time_col='time'):\n",
    "    \"\"\"\n",
    "    Extract temporal features from Unix timestamp (milliseconds).\n",
    "    \n",
    "    Returns DataFrame with new temporal columns.\n",
    "    \"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Convert milliseconds to datetime\n",
    "    result['datetime'] = pd.to_datetime(result[time_col], unit='ms', errors='coerce')\n",
    "    \n",
    "    # Basic temporal features\n",
    "    result['hour_of_day'] = result['datetime'].dt.hour\n",
    "    result['day_of_week'] = result['datetime'].dt.dayofweek  # 0=Monday\n",
    "    result['is_weekend'] = (result['day_of_week'] >= 5).astype(int)\n",
    "    result['is_business_hours'] = ((result['hour_of_day'] >= 9) & \n",
    "                                    (result['hour_of_day'] < 17)).astype(int)\n",
    "    \n",
    "    # Cyclical encoding (sin/cos) - preserves circular nature\n",
    "    result['hour_sin'] = np.sin(2 * np.pi * result['hour_of_day'] / 24)\n",
    "    result['hour_cos'] = np.cos(2 * np.pi * result['hour_of_day'] / 24)\n",
    "    result['day_sin'] = np.sin(2 * np.pi * result['day_of_week'] / 7)\n",
    "    result['day_cos'] = np.cos(2 * np.pi * result['day_of_week'] / 7)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Apply temporal feature extraction\n",
    "df = extract_temporal_features(df)\n",
    "\n",
    "# Show sample of temporal features\n",
    "temporal_cols = ['datetime', 'hour_of_day', 'day_of_week', 'is_weekend', \n",
    "                 'is_business_hours', 'hour_sin', 'hour_cos']\n",
    "df[temporal_cols].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Hour distribution\n",
    "df['hour_of_day'].hist(bins=24, ax=axes[0, 0], edgecolor='black', color='steelblue')\n",
    "axes[0, 0].set_xlabel('Hour of Day')\n",
    "axes[0, 0].set_ylabel('Count')\n",
    "axes[0, 0].set_title('Event Distribution by Hour')\n",
    "axes[0, 0].set_xticks(range(0, 24, 2))\n",
    "\n",
    "# Day of week distribution\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "day_counts = df['day_of_week'].value_counts().sort_index()\n",
    "axes[0, 1].bar(day_names, [day_counts.get(i, 0) for i in range(7)], \n",
    "               edgecolor='black', color='steelblue')\n",
    "axes[0, 1].set_xlabel('Day of Week')\n",
    "axes[0, 1].set_ylabel('Count')\n",
    "axes[0, 1].set_title('Event Distribution by Day')\n",
    "\n",
    "# Cyclical encoding visualization\n",
    "hours = np.arange(24)\n",
    "hour_sin = np.sin(2 * np.pi * hours / 24)\n",
    "hour_cos = np.cos(2 * np.pi * hours / 24)\n",
    "axes[1, 0].plot(hours, hour_sin, 'b-', label='sin(hour)', linewidth=2)\n",
    "axes[1, 0].plot(hours, hour_cos, 'r-', label='cos(hour)', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Encoded Value')\n",
    "axes[1, 0].set_title('Cyclical Hour Encoding')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].set_xticks(range(0, 24, 4))\n",
    "axes[1, 0].axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show why cyclical encoding matters\n",
    "axes[1, 1].scatter(hour_sin, hour_cos, c=hours, cmap='hsv', s=100)\n",
    "for i in [0, 6, 12, 18, 23]:\n",
    "    axes[1, 1].annotate(f'{i}h', (hour_sin[i], hour_cos[i]), \n",
    "                        textcoords=\"offset points\", xytext=(5, 5))\n",
    "axes[1, 1].set_xlabel('hour_sin')\n",
    "axes[1, 1].set_ylabel('hour_cos')\n",
    "axes[1, 1].set_title('Hours in (sin, cos) Space\\n(Note: 23h and 0h are adjacent!)')\n",
    "axes[1, 1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### How to read these charts\n\n**Top row - Event distributions:**\n- **Left (Hour)**: Shows when events occurred. A single spike means all events happened at the same hour. In production data, you'd expect events spread across hours with patterns (e.g., more during business hours).\n- **Right (Day)**: Shows which days events occurred. A single bar means all events are from one day.\n\n**Bottom row - Cyclical encoding explained:**\n- **Left (sin/cos curves)**: Shows how hours map to sin/cos values. Notice that hour 0 and hour 23 have similar values - this is intentional! It captures that midnight and 11 PM are temporally close.\n- **Right (circular plot)**: Each dot is an hour plotted as (sin, cos) coordinates. Hours form a circle, so 23h and 0h are adjacent - unlike raw hour values where they'd be 23 units apart.\n\n**Note on this sample data:** The synthetic dataset was generated at a single point in time (all events show Tuesday at 3 PM). This means temporal features have zero variance and won't contribute to anomaly detection. In production OCSF data collected over days/weeks, you'd see realistic temporal distributions where time-of-day patterns become powerful anomaly signals.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Select Core Features\n\nNot all 60+ columns are useful for embedding. We select:\n- **Categorical**: class, activity, status, severity, user, HTTP method, URL path, response code\n- **Numerical**: duration, temporal features (including cyclical encodings)\n\n**Categorical vs Numerical decision criteria:**\n- **Categorical**: Discrete codes/IDs where numerical distance is meaningless\n  - `http_response_code`: 200, 404, 500 are status classes, not quantities\n  - `severity_id`, `activity_id`, `status_id`: OCSF ID codes, not continuous values\n- **Numerical**: Continuous values or derived features where math makes sense\n  - `duration`: Actual time measurement\n  - `is_weekend`, `is_business_hours`: Binary flags (0/1 works fine)\n  - `hour_sin`, `hour_cos`: Continuous cyclical encodings\n\n**What you should expect:**\n- 12 categorical features with varying cardinalities\n- 8 numerical features (duration + temporal)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define feature sets\n#\n# Categorical: Discrete codes/IDs where numerical distance is meaningless\n# - http_response_code: 200, 404, 500 are status classes\n# - severity_id, activity_id, status_id: OCSF ID codes (not continuous values)\n#\n# Numerical: Continuous values where arithmetic makes sense\n# - duration: actual time measurement\n# - Binary flags and cyclical encodings work fine as numeric\n\ncategorical_features = [\n    'class_name',\n    'activity_name',\n    'status',\n    'level',\n    'service',\n    'actor_user_name',\n    'http_request_method',\n    'http_request_url_path',\n    'http_response_code',   # Discrete status classes (200, 404, 500)\n    'severity_id',          # OCSF severity levels (1=Info, 2=Low, 3=Medium, etc.)\n    'activity_id',          # OCSF activity type codes\n    'status_id',            # OCSF status codes (0=Unknown, 1=Success, 2=Failure)\n]\n\nnumerical_features = [\n    'duration',             # Continuous: actual time measurement\n    'hour_of_day',          # Used for cyclical encoding\n    'day_of_week',          # Used for cyclical encoding\n    'is_weekend',           # Binary flag (0/1)\n    'is_business_hours',    # Binary flag (0/1)\n    'hour_sin',             # Continuous cyclical encoding\n    'hour_cos',             # Continuous cyclical encoding\n    'day_sin',              # Continuous cyclical encoding\n    'day_cos',              # Continuous cyclical encoding\n]\n\n# Filter to columns that exist in our data\ncategorical_features = [c for c in categorical_features if c in df.columns]\nnumerical_features = [c for c in numerical_features if c in df.columns]\n\nprint(f\"Selected Features:\")\nprint(f\"\\nCategorical ({len(categorical_features)}):\")\nfor col in categorical_features:\n    print(f\"  - {col}: {df[col].nunique()} unique values\")\n    \nprint(f\"\\nNumerical ({len(numerical_features)}):\")\nfor col in numerical_features:\n    print(f\"  - {col}: range [{df[col].min():.1f}, {df[col].max():.1f}]\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Handle Missing Values\n",
    "\n",
    "OCSF events have **optional fields**. Our strategy:\n",
    "- **Categorical**: Use special 'MISSING' category\n",
    "- **Numerical**: Use 0 (or median for important features)\n",
    "\n",
    "**What you should expect:**\n",
    "- After handling, no nulls should remain\n",
    "- Some categorical columns may show 'MISSING' as a frequent value\n",
    "\n",
    "**If you still see nulls:**\n",
    "- Check if new columns were added after the missing value handling\n",
    "- Ensure the column list matches what's in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values before handling\n",
    "all_features = categorical_features + numerical_features\n",
    "missing_before = df[all_features].isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0]\n",
    "\n",
    "if len(missing_before) > 0:\n",
    "    print(\"Missing values BEFORE handling:\")\n",
    "    print(missing_before.to_frame('null_count'))\n",
    "else:\n",
    "    print(\"No missing values in selected features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def handle_missing_values(df, categorical_cols, numerical_cols):\n    \"\"\"\n    Handle missing values in feature columns.\n    \"\"\"\n    result = df.copy()\n    \n    # Categorical: fill with 'MISSING' and convert to string\n    # This handles numeric columns like http_response_code correctly\n    for col in categorical_cols:\n        if col in result.columns:\n            # Convert to string first (handles numeric categoricals like http_response_code)\n            result[col] = result[col].astype(str)\n            result[col] = result[col].replace('nan', 'MISSING').replace('', 'MISSING')\n    \n    # Numerical: fill with 0\n    for col in numerical_cols:\n        if col in result.columns:\n            result[col] = pd.to_numeric(result[col], errors='coerce').fillna(0)\n    \n    return result\n\n# Apply missing value handling\ndf_clean = handle_missing_values(df, categorical_features, numerical_features)\n\n# Verify no nulls remain\nnull_counts = df_clean[all_features].isnull().sum()\nif null_counts.sum() > 0:\n    print(\"WARNING: Nulls remaining after handling:\")\n    print(null_counts[null_counts > 0])\nelse:\n    print(\"Success: No nulls remaining in feature columns.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Features for TabularResNet\n",
    "\n",
    "TabularResNet needs:\n",
    "- **Numerical array**: Normalized floats (mean=0, std=1)\n",
    "- **Categorical array**: Integer indices (0, 1, 2, ...)\n",
    "\n",
    "**What you should expect:**\n",
    "- Numerical values centered around 0 with std close to 1\n",
    "- Categorical values as non-negative integers\n",
    "- Cardinalities = vocabulary size + 1 (for UNKNOWN)\n",
    "\n",
    "**If you see unexpected values:**\n",
    "- Very large numerical values: Check if outliers need handling\n",
    "- Negative categorical values: Should not happen with LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "def prepare_for_tabular_resnet(df, categorical_cols, numerical_cols):\n",
    "    \"\"\"\n",
    "    Prepare features for TabularResNet.\n",
    "    \n",
    "    Returns:\n",
    "        numerical_array: Normalized numerical features\n",
    "        categorical_array: Integer-encoded categorical features\n",
    "        encoders: Dict of LabelEncoders\n",
    "        scaler: StandardScaler\n",
    "        cardinalities: List of vocab sizes per categorical\n",
    "    \"\"\"\n",
    "    # Encode categorical features\n",
    "    encoders = {}\n",
    "    categorical_data = []\n",
    "    cardinalities = []\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        encoder = LabelEncoder()\n",
    "        # Add 'UNKNOWN' for handling new values at inference\n",
    "        unique_vals = list(df[col].unique()) + ['UNKNOWN']\n",
    "        encoder.fit(unique_vals)\n",
    "        encoded = encoder.transform(df[col])\n",
    "        categorical_data.append(encoded)\n",
    "        encoders[col] = encoder\n",
    "        cardinalities.append(len(encoder.classes_))\n",
    "    \n",
    "    categorical_array = np.column_stack(categorical_data) if categorical_data else np.array([])\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_array = scaler.fit_transform(df[numerical_cols])\n",
    "    \n",
    "    return numerical_array, categorical_array, encoders, scaler, cardinalities\n",
    "\n",
    "# Prepare features\n",
    "numerical_array, categorical_array, encoders, scaler, cardinalities = \\\n",
    "    prepare_for_tabular_resnet(df_clean, categorical_features, numerical_features)\n",
    "\n",
    "print(\"Feature Arrays Ready for TabularResNet:\")\n",
    "print(f\"  Numerical shape: {numerical_array.shape}\")\n",
    "print(f\"  Categorical shape: {categorical_array.shape}\")\n",
    "print(f\"\\nCategorical Cardinalities (vocab size + UNKNOWN):\")\n",
    "for col, card in zip(categorical_features, cardinalities):\n",
    "    print(f\"  {col}: {card}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview numerical features (normalized)\n",
    "print(\"Numerical features (first 5 rows, normalized):\")\n",
    "print(\"Expected: values centered around 0, mostly between -3 and 3\")\n",
    "print()\n",
    "pd.DataFrame(numerical_array[:5], columns=numerical_features).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview categorical features (integer encoded)\n",
    "print(\"Categorical features (first 5 rows, integer encoded):\")\n",
    "print(\"Expected: non-negative integers (0 to cardinality-1)\")\n",
    "print()\n",
    "pd.DataFrame(categorical_array[:5], columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize numerical feature distributions after scaling\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (col, ax) in enumerate(zip(numerical_features[:8], axes)):\n",
    "    ax.hist(numerical_array[:, i], bins=50, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(0, color='red', linestyle='--', label='mean=0')\n",
    "    ax.set_title(col)\n",
    "    ax.set_xlabel('Normalized Value')\n",
    "\n",
    "plt.suptitle('Numerical Feature Distributions (After StandardScaler)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Encoding Quality\n",
    "\n",
    "Before saving, let's verify the encoding is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify scaler statistics\n",
    "print(\"Scaler Statistics (should show diverse ranges before scaling):\")\n",
    "print()\n",
    "scaler_stats = pd.DataFrame({\n",
    "    'feature': numerical_features,\n",
    "    'original_mean': scaler.mean_,\n",
    "    'original_std': scaler.scale_\n",
    "}).round(4)\n",
    "scaler_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify categorical encoding can handle UNKNOWN values\n",
    "print(\"Testing UNKNOWN handling for categorical encoders:\")\n",
    "print()\n",
    "for col, encoder in encoders.items():\n",
    "    # Check UNKNOWN is in classes\n",
    "    has_unknown = 'UNKNOWN' in encoder.classes_\n",
    "    unknown_idx = encoder.transform(['UNKNOWN'])[0] if has_unknown else None\n",
    "    print(f\"  {col}: UNKNOWN index = {unknown_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Processed Features\n",
    "\n",
    "Save the processed data and encoding artifacts for training.\n",
    "\n",
    "**Files saved:**\n",
    "- `numerical_features.npy`: Normalized numerical features\n",
    "- `categorical_features.npy`: Integer-encoded categorical features\n",
    "- `feature_artifacts.pkl`: Encoders, scaler, column names, cardinalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save feature arrays\n",
    "np.save('../data/numerical_features.npy', numerical_array)\n",
    "np.save('../data/categorical_features.npy', categorical_array)\n",
    "\n",
    "# Save encoders and scaler\n",
    "artifacts = {\n",
    "    'encoders': encoders,\n",
    "    'scaler': scaler,\n",
    "    'categorical_cols': categorical_features,\n",
    "    'numerical_cols': numerical_features,\n",
    "    'cardinalities': cardinalities\n",
    "}\n",
    "\n",
    "with open('../data/feature_artifacts.pkl', 'wb') as f:\n",
    "    pickle.dump(artifacts, f)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(f\"  - numerical_features.npy: {numerical_array.shape}\")\n",
    "print(f\"  - categorical_features.npy: {categorical_array.shape}\")\n",
    "print(f\"  - feature_artifacts.pkl: encoders + scaler + metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nIn this notebook, we:\n\n1. **Loaded OCSF data** from parquet format (~3000+ events)\n2. **Explored the schema** - 60+ columns with nested objects flattened\n3. **Engineered temporal features** - hour, day, cyclical sin/cos encoding\n4. **Selected core features** - 12 categorical + 9 numerical\n5. **Handled missing values** - 'MISSING' for categorical, 0 for numerical\n6. **Encoded for TabularResNet** - LabelEncoder + StandardScaler\n\n**Design decision**: OCSF ID fields are treated as categorical:\n- `http_response_code`, `severity_id`, `activity_id`, `status_id` are discrete codes\n- Embedding layers learn semantic relationships between codes\n- Only truly continuous values (duration, cyclical encodings) are numerical\n\n**Key outputs:**\n- Numerical array: `(N, num_features)` normalized floats\n- Categorical array: `(N, cat_features)` integer indices\n- Artifacts: Encoders and scaler for inference\n\n**Next**: Use these features in [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb) to train embeddings."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}