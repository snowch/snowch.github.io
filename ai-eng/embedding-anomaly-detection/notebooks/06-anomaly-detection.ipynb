{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Anomaly Detection Methods\n",
    "\n",
    "Apply anomaly detection algorithms to OCSF embeddings.\n",
    "\n",
    "**What you'll learn:**\n",
    "1. Distance-based anomaly detection (k-NN)\n",
    "2. Density-based detection (Local Outlier Factor)\n",
    "3. Tree-based detection (Isolation Forest)\n",
    "4. Evaluating detection performance\n",
    "5. Comparing methods on labeled evaluation subset\n",
    "\n",
    "**Prerequisites:**\n",
    "- Embeddings from [04-self-supervised-training.ipynb](04-self-supervised-training.ipynb)\n",
    "- Labeled evaluation subset (optional, for evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import LocalOutlierFactor, NearestNeighbors\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For nicer plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Embeddings and Labels\n",
    "\n",
    "Load the embeddings from training and the labeled evaluation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings\n",
    "embeddings = np.load('../data/embeddings.npy')\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "# Load labeled evaluation subset (if available)\n",
    "try:\n",
    "    eval_df = pd.read_parquet('../data/ocsf_eval_subset.parquet')\n",
    "    print(f\"Evaluation subset: {len(eval_df)} events\")\n",
    "    print(f\"Anomaly rate: {eval_df['is_anomaly'].mean():.2%}\")\n",
    "    has_labels = True\n",
    "except FileNotFoundError:\n",
    "    print(\"No labeled evaluation subset found. Will use unsupervised evaluation.\")\n",
    "    has_labels = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-NN Distance-Based Detection\n",
    "\n",
    "**Idea**: Anomalies are far from their nearest neighbors.\n",
    "\n",
    "For each point:\n",
    "1. Find k nearest neighbors\n",
    "2. Compute average distance to neighbors\n",
    "3. High average distance = likely anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_knn_distance(embeddings, k=20, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect anomalies using k-NN average distance.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: (N, d) array of embeddings\n",
    "        k: Number of neighbors\n",
    "        contamination: Expected anomaly proportion\n",
    "    \n",
    "    Returns:\n",
    "        predictions: 1 for anomaly, 0 for normal\n",
    "        scores: Average distance to k neighbors (higher = more anomalous)\n",
    "    \"\"\"\n",
    "    # Fit k-NN model\n",
    "    nn = NearestNeighbors(n_neighbors=k+1, metric='cosine')  # +1 because point is its own neighbor\n",
    "    nn.fit(embeddings)\n",
    "    \n",
    "    # Get distances to k nearest neighbors\n",
    "    distances, _ = nn.kneighbors(embeddings)\n",
    "    \n",
    "    # Average distance (excluding self)\n",
    "    scores = distances[:, 1:].mean(axis=1)\n",
    "    \n",
    "    # Threshold at percentile\n",
    "    threshold = np.percentile(scores, 100 * (1 - contamination))\n",
    "    predictions = (scores > threshold).astype(int)\n",
    "    \n",
    "    return predictions, scores, threshold\n",
    "\n",
    "# Run k-NN detection\n",
    "knn_preds, knn_scores, knn_threshold = detect_anomalies_knn_distance(\n",
    "    embeddings, k=20, contamination=0.05\n",
    ")\n",
    "\n",
    "print(f\"k-NN Distance Detection:\")\n",
    "print(f\"  Threshold: {knn_threshold:.4f}\")\n",
    "print(f\"  Anomalies detected: {knn_preds.sum()} ({knn_preds.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot score distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram of scores\n",
    "axes[0].hist(knn_scores, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(knn_threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {knn_threshold:.4f}')\n",
    "axes[0].set_xlabel('Average k-NN Distance')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('k-NN Distance Score Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Sorted scores\n",
    "sorted_scores = np.sort(knn_scores)[::-1]\n",
    "axes[1].plot(sorted_scores, linewidth=1)\n",
    "axes[1].axhline(knn_threshold, color='red', linestyle='--', linewidth=2, label='Threshold')\n",
    "axes[1].set_xlabel('Rank')\n",
    "axes[1].set_ylabel('k-NN Distance Score')\n",
    "axes[1].set_title('Sorted Anomaly Scores')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Local Outlier Factor (LOF)\n",
    "\n",
    "**Idea**: Anomalies are in regions of lower density than their neighbors.\n",
    "\n",
    "LOF adapts to local density - a point can be far from clusters but still normal if its local area has similar density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_lof(embeddings, n_neighbors=20, contamination=0.05):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Local Outlier Factor.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: (N, d) array of embeddings\n",
    "        n_neighbors: Number of neighbors for density estimation\n",
    "        contamination: Expected anomaly proportion\n",
    "    \n",
    "    Returns:\n",
    "        predictions: 1 for anomaly, 0 for normal\n",
    "        scores: Negative outlier factor (more negative = more anomalous)\n",
    "    \"\"\"\n",
    "    lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n",
    "    lof_predictions = lof.fit_predict(embeddings)\n",
    "    \n",
    "    # Convert: LOF returns -1 for anomalies, 1 for normal\n",
    "    predictions = (lof_predictions == -1).astype(int)\n",
    "    \n",
    "    # Scores (negative_outlier_factor_ is more negative for anomalies)\n",
    "    scores = -lof.negative_outlier_factor_  # Flip so higher = more anomalous\n",
    "    \n",
    "    return predictions, scores\n",
    "\n",
    "# Run LOF detection\n",
    "lof_preds, lof_scores = detect_anomalies_lof(embeddings, n_neighbors=20, contamination=0.05)\n",
    "\n",
    "print(f\"Local Outlier Factor (LOF) Detection:\")\n",
    "print(f\"  Anomalies detected: {lof_preds.sum()} ({lof_preds.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Isolation Forest\n",
    "\n",
    "**Idea**: Anomalies are easier to \"isolate\" with random splits.\n",
    "\n",
    "Build random trees that recursively split data. Anomalies require fewer splits to isolate (shorter path length)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_isolation_forest(embeddings, contamination=0.05, n_estimators=100):\n",
    "    \"\"\"\n",
    "    Detect anomalies using Isolation Forest.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: (N, d) array of embeddings\n",
    "        contamination: Expected anomaly proportion\n",
    "        n_estimators: Number of trees\n",
    "    \n",
    "    Returns:\n",
    "        predictions: 1 for anomaly, 0 for normal\n",
    "        scores: Anomaly score (higher = more anomalous)\n",
    "    \"\"\"\n",
    "    iso = IsolationForest(contamination=contamination, n_estimators=n_estimators, random_state=42)\n",
    "    iso_predictions = iso.fit_predict(embeddings)\n",
    "    \n",
    "    # Convert: Isolation Forest returns -1 for anomalies, 1 for normal\n",
    "    predictions = (iso_predictions == -1).astype(int)\n",
    "    \n",
    "    # Scores (score_samples returns negative values, more negative = more anomalous)\n",
    "    scores = -iso.score_samples(embeddings)  # Flip so higher = more anomalous\n",
    "    \n",
    "    return predictions, scores\n",
    "\n",
    "# Run Isolation Forest detection\n",
    "iso_preds, iso_scores = detect_anomalies_isolation_forest(embeddings, contamination=0.05)\n",
    "\n",
    "print(f\"Isolation Forest Detection:\")\n",
    "print(f\"  Anomalies detected: {iso_preds.sum()} ({iso_preds.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Detection Methods\n",
    "\n",
    "If we have labeled evaluation data, we can compare method performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_detector(true_labels, predictions, scores, method_name):\n",
    "    \"\"\"\n",
    "    Evaluate detection performance.\n",
    "    \"\"\"\n",
    "    precision = precision_score(true_labels, predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, predictions, zero_division=0)\n",
    "    f1 = f1_score(true_labels, predictions, zero_division=0)\n",
    "    \n",
    "    # ROC AUC (if we have scores)\n",
    "    try:\n",
    "        auc = roc_auc_score(true_labels, scores)\n",
    "    except:\n",
    "        auc = 0.0\n",
    "    \n",
    "    return {\n",
    "        'Method': method_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1': f1,\n",
    "        'AUC': auc\n",
    "    }\n",
    "\n",
    "# If we have labels, evaluate\n",
    "if has_labels:\n",
    "    # Note: Evaluation subset may have different indices than full embeddings\n",
    "    # For proper evaluation, you'd need to match indices or generate embeddings for eval subset\n",
    "    print(\"Using labeled evaluation subset for comparison...\")\n",
    "    \n",
    "    # For demo, we'll create synthetic labels based on scores\n",
    "    # In practice, match your evaluation subset to embeddings\n",
    "    n_eval = min(len(eval_df), len(embeddings))\n",
    "    \n",
    "    # Use the eval_df labels if indices match\n",
    "    if 'is_anomaly' in eval_df.columns:\n",
    "        true_labels = eval_df['is_anomaly'].values[:n_eval]\n",
    "        \n",
    "        # Evaluate each method (on first n_eval samples)\n",
    "        results = []\n",
    "        results.append(evaluate_detector(true_labels, knn_preds[:n_eval], knn_scores[:n_eval], 'k-NN Distance'))\n",
    "        results.append(evaluate_detector(true_labels, lof_preds[:n_eval], lof_scores[:n_eval], 'LOF'))\n",
    "        results.append(evaluate_detector(true_labels, iso_preds[:n_eval], iso_scores[:n_eval], 'Isolation Forest'))\n",
    "        \n",
    "        results_df = pd.DataFrame(results)\n",
    "        print(\"\\nMethod Comparison:\")\n",
    "        print(results_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No labels available. Showing unsupervised comparison.\")\n",
    "    \n",
    "    # Compare agreement between methods\n",
    "    print(f\"\\nMethod Agreement:\")\n",
    "    print(f\"  k-NN & LOF agree:      {(knn_preds == lof_preds).mean():.2%}\")\n",
    "    print(f\"  k-NN & IsoForest agree: {(knn_preds == iso_preds).mean():.2%}\")\n",
    "    print(f\"  LOF & IsoForest agree:  {(lof_preds == iso_preds).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Anomalies\n",
    "\n",
    "Plot anomaly scores and highlight detected anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare score distributions across methods\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "methods = [\n",
    "    ('k-NN Distance', knn_scores, knn_preds),\n",
    "    ('LOF', lof_scores, lof_preds),\n",
    "    ('Isolation Forest', iso_scores, iso_preds)\n",
    "]\n",
    "\n",
    "for ax, (name, scores, preds) in zip(axes, methods):\n",
    "    # Plot normal vs anomaly score distributions\n",
    "    normal_scores = scores[preds == 0]\n",
    "    anomaly_scores = scores[preds == 1]\n",
    "    \n",
    "    ax.hist(normal_scores, bins=30, alpha=0.7, label='Normal', color='blue')\n",
    "    ax.hist(anomaly_scores, bins=30, alpha=0.7, label='Anomaly', color='red')\n",
    "    ax.set_xlabel('Anomaly Score')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(name)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble: combine multiple methods\n",
    "def ensemble_detection(predictions_list, threshold=2):\n",
    "    \"\"\"\n",
    "    Ensemble detection: flag as anomaly if >= threshold methods agree.\n",
    "    \"\"\"\n",
    "    votes = np.sum(predictions_list, axis=0)\n",
    "    return (votes >= threshold).astype(int)\n",
    "\n",
    "# Combine all three methods\n",
    "ensemble_preds = ensemble_detection([knn_preds, lof_preds, iso_preds], threshold=2)\n",
    "\n",
    "print(f\"Ensemble Detection (2/3 agreement):\")\n",
    "print(f\"  Anomalies detected: {ensemble_preds.sum()} ({ensemble_preds.mean():.2%})\")\n",
    "\n",
    "# Venn diagram of method overlap\n",
    "print(f\"\\nMethod Overlap:\")\n",
    "print(f\"  Only k-NN:          {((knn_preds == 1) & (lof_preds == 0) & (iso_preds == 0)).sum()}\")\n",
    "print(f\"  Only LOF:           {((knn_preds == 0) & (lof_preds == 1) & (iso_preds == 0)).sum()}\")\n",
    "print(f\"  Only IsoForest:     {((knn_preds == 0) & (lof_preds == 0) & (iso_preds == 1)).sum()}\")\n",
    "print(f\"  All three agree:    {((knn_preds == 1) & (lof_preds == 1) & (iso_preds == 1)).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inspect Top Anomalies\n",
    "\n",
    "Look at the events with highest anomaly scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original data for inspection\n",
    "df = pd.read_parquet('../data/ocsf_logs.parquet')\n",
    "\n",
    "# Add anomaly scores\n",
    "df = df.iloc[:len(knn_scores)].copy()  # Match lengths\n",
    "df['knn_score'] = knn_scores[:len(df)]\n",
    "df['lof_score'] = lof_scores[:len(df)]\n",
    "df['iso_score'] = iso_scores[:len(df)]\n",
    "df['ensemble_anomaly'] = ensemble_preds[:len(df)]\n",
    "\n",
    "# Top anomalies by k-NN score\n",
    "print(\"Top 10 Anomalies by k-NN Distance Score:\")\n",
    "top_cols = ['message', 'service', 'level', 'knn_score', 'lof_score', 'iso_score']\n",
    "top_cols = [c for c in top_cols if c in df.columns]\n",
    "display_df = df.nlargest(10, 'knn_score')[top_cols]\n",
    "print(display_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results\n",
    "\n",
    "Save anomaly predictions for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save anomaly predictions\n",
    "results = pd.DataFrame({\n",
    "    'knn_score': knn_scores,\n",
    "    'knn_anomaly': knn_preds,\n",
    "    'lof_score': lof_scores,\n",
    "    'lof_anomaly': lof_preds,\n",
    "    'iso_score': iso_scores,\n",
    "    'iso_anomaly': iso_preds,\n",
    "    'ensemble_anomaly': ensemble_preds\n",
    "})\n",
    "\n",
    "results.to_parquet('../data/anomaly_predictions.parquet')\n",
    "print(\"Saved anomaly predictions to ../data/anomaly_predictions.parquet\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary:\")\n",
    "print(results.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **k-NN Distance**: Detected anomalies based on average distance to neighbors\n",
    "2. **LOF**: Used local density comparison for adaptive detection\n",
    "3. **Isolation Forest**: Leveraged tree-based isolation for anomaly scoring\n",
    "4. **Ensemble**: Combined methods for robust detection\n",
    "5. **Evaluated**: Compared methods on labeled subset (if available)\n",
    "\n",
    "**Key insights**:\n",
    "- Different methods catch different types of anomalies\n",
    "- Ensemble voting (2/3 agreement) reduces false positives\n",
    "- LOF adapts to varying local densities in the data\n",
    "- k-NN distance is simple but effective for global outliers\n",
    "\n",
    "**Production considerations**:\n",
    "- Use vector database (FAISS, Milvus) for efficient k-NN at scale\n",
    "- Tune `contamination` based on expected anomaly rate\n",
    "- Monitor detection performance over time (concept drift)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
